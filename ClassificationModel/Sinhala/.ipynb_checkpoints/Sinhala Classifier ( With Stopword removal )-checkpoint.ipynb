{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiying Documents - SInhala (With StopWord removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Profiling\n",
    "checks and to get an idea of how different features of the data are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from pandas import DataFrame,Series\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv file as a dataframe\n",
    "df=pd.read_csv(\"SinhalaProcessedNew1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data-set has 2944 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"The data-set has %d rows and %d columns\"%(df.shape[0],df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['content', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['content', 'category'], dtype='object')\n",
      "Missing values\n",
      "content: 9\n",
      "category: 0\n"
     ]
    }
   ],
   "source": [
    "#data-sets usually have missing values in them for a variety of reasons. In Numpy, missing values are represented as NaN and \n",
    "\n",
    "from __future__ import print_function #my current version of python doesn't have the functionality that I intend to use in the\n",
    "#following lines of codes so thus importing fresh and new print function \"from the future\"\n",
    "print (df.columns) \n",
    "\n",
    "#to calculate number of missing values in each column. True values are coerced as 1 and False as 0 and thus I used that\n",
    "#fact in \"sum\" function to calculate how many missing values are there in each column:\n",
    "print(\"Missing values\")\n",
    "for col_name in df.columns:\n",
    "    print (col_name,end=\": \")\n",
    "    print (sum(df[col_name].isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([140, 240, 1088, 1574, 1680, 1701, 1946, 1969, 2038], dtype='int64')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Identifi rows that contain null values\n",
    "print(df.ix[df['content'].isnull()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can handle missing value in many ways\n",
    "1. Either we remove the row which has missing value\n",
    "2. Or we will \"imputate\" the missing value\n",
    "3. Or a more simpler approach: Knowing that the \"text_description\" column contains text (str) so we can reload the data-set with this knowledge and clearly specifying that in str based missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the unzipped file here to demonstrate another usage of read_csv function:\n",
    "\n",
    "df2=pd.read_csv('SinhalaProcessedNew1.csv',index_col=False,na_values='',na_filter=True)\n",
    "df2.columns=['content', 'category']\n",
    "sum(df2.content.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df2.dropna(subset=['content'])\n",
    "sum(df2.content.isnull()) #again checking that whether there are still missing values in that column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicate values in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df2.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>වැන්ටේජ් එෆ්.ඒ කුසලාන පාපන්දු තරගාවලියේ සතිය ත...</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>ඩයලොග් ශූරයන්ගේ කුසලාන පාපන්දු තරගාවලියේදී සෝන...</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  category\n",
       "2634  වැන්ටේජ් එෆ්.ඒ කුසලාන පාපන්දු තරගාවලියේ සතිය ත...  football\n",
       "2767  ඩයලොග් ශූරයන්ගේ කුසලාන පාපන්දු තරගාවලියේදී සෝන...  football"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see which rows are duplicated:\n",
    "df2.loc[df2.duplicated(keep='first'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2935, 2)\n"
     ]
    }
   ],
   "source": [
    "#creating a new dataframe with the duplicate rows removed from the original dataframe. We can also use drop_duplicate(inplace=True)\n",
    "#parameter as well.\n",
    "print(df2.shape)\n",
    "df2=df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2933, 2)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now verifying whether there are still duplicate values in our dataframe or not:\n",
    "sum(df2.duplicated()) \n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df2.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the Value Distribution profile of the Category Variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just reassigning somewhat smaller column names for convenience. \n",
    "df2.columns=['content', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counter={x:0 for x in set(df2['category'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'football': 853, 'cricket': 1029, 'rugby': 1051}\n"
     ]
    }
   ],
   "source": [
    "for each_cat in df2['category']:\n",
    "    category_counter[each_cat]+=1\n",
    "    \n",
    "print(category_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.bar(range(len(category_counter)),category_counter.values(),align='center',tick_label=[\"cricket\",\"rugby\",\"football\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Analysis of the Text Data:\n",
    "As text data's preprocessing transforms the words/terms into respective columns (though in much improved approach e.g. only frequent terms and also the rare ones (TFIDF) are transformed into columns, more on that later), so its a good step to analyze the vocabulary richness of our data. If we just analyze the lexical richness of the text_description data, it will still give us much idea about how rich our data is in terms of unique vocabulary words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=df2.content\n",
    "# corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************Before this do our pre prosessing**************************\n",
    "\n",
    "corpus=df2['content'].values.astype('unicode')\n",
    "#corpus means collection of text. For this particular data-set, I will treat the newly created column title_desc\n",
    "#as my corpus and will use that to create features.\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#Initializing TFIDF vectorizer to conver the raw corpus to a matrix of TFIDF features and also enabling the removal of stopwords.\n",
    "tfidf_matrix=vectorizer.fit_transform(corpus).todense()\n",
    "#creating TFIDF features sparse matrix by fitting it on the specified corpus. \n",
    "tfidf_names=vectorizer.get_feature_names()\n",
    "#grabbing the name of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TFIDF Features: 5862\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of TFIDF Features: %d\"%len(tfidf_names)) #same info can be gathered by using tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which shows that there are 5816 columns that will be used for training the classifier. These are much smaller than the total\n",
    "#number of unique vocabulary words that are there (as we calculated previously) in the text description column alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time_container={'b_naive_bayes':0,'mn_naive_bayes':0,'random_forest':0,'linear_svm':0}\n",
    "prediction_time_container={'b_naive_bayes':0,'mn_naive_bayes':0,'random_forest':0,'linear_svm':0}\n",
    "\n",
    "accuracy_container={'b_naive_bayes':0,'mn_naive_bayes':0,'random_forest':0,'linear_svm':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Classifiers, Making Predictions and Validating Results\n",
    "With the data preprocessed, now is the time to develop the models. When it comes to developing machine learning models (and in our particular case, classifiers), we need to firstly train them on the labeled training data to learn from and then use the test data-set to make predictions. So to do that, we will proceed with splitting our existing data-set into training and test data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As no separate test data-set was given so the provided data set is split into training and test data set using 70-30% ratio \n",
    "#as follows:\n",
    "variables = tfidf_matrix\n",
    "#considering the TFIDF features as independent variables to be input to the classifier.\n",
    "labels = df2.category\n",
    "#considering the category values as the class labels for the classifier.\n",
    "\n",
    "variables_train, variables_test, labels_train, labels_test  =   train_test_split(variables, labels, test_size=.3)\n",
    "#splitting the data into random training and test sets for both independent variables and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data: (2053, 5862)\n",
      "Shape of Test Data: (880, 5862)\n"
     ]
    }
   ],
   "source": [
    "#analyzing the shape of the training and test data-set:\n",
    "print('Shape of Training Data: '+str(variables_train.shape))\n",
    "print('Shape of Test Data: '+str(variables_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Naive Bayes:\n",
    "Naive Bayes is one of the most widely used classification algorithm in text mining applications. Based on Bayes theorem, this model makes the assumption that all the features are independent of each other and uses the probabilities of each attribute belonging to each class to make a prediction. The condition of independence may not be valid in many circumstances but as a base line model, its a good starting point to test its performance on the provided data. There are two forms of Naive Bayes:\n",
    "\n",
    "Bernoulli (designed for boolean/binary features i.e. just considers the presence or absense of a feature)\n",
    "Multinomial (which also considers the occurrence counts of the feature)\n",
    "We will apply both and then will assess their respective accuracy scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['b_naive_bayes', 'mn_naive_bayes', 'random_forest', 'linear_svm'])"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_time_container.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#loading Gaussian Naive Bayes from the sklearn library:\n",
    "bnb_classifier=BernoulliNB()\n",
    "#initializing the object\n",
    "t0=time()\n",
    "bnb_classifier=bnb_classifier.fit(variables_train,labels_train)\n",
    "training_time_container['b_naive_bayes']=time()-t0\n",
    "#fitting the classifier or training the classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after the model has been trained, we proceed to test its performance on the test data:\n",
    "t0=time()\n",
    "bnb_predictions=bnb_classifier.predict(variables_test)\n",
    "prediction_time_container['b_naive_bayes']=time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15027689933776855"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_time_container['b_naive_bayes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the trained classifier has been used to make predictions on the test data-set. To evaluate the performance of the model,\n",
    "#there are a number of metrics that can be used as follows:\n",
    "nb_ascore=sklearn.metrics.accuracy_score(labels_test, bnb_predictions)\n",
    "accuracy_container['b_naive_bayes']=nb_ascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy Score: 0.897727\n",
      "Training Time: 0.263073\n",
      "Prediction Time: 0.150277\n"
     ]
    }
   ],
   "source": [
    "print(\"Bernoulli Naive Bayes Accuracy Score: %f\"%accuracy_container['b_naive_bayes'])\n",
    "print(\"Training Time: %f\"%training_time_container['b_naive_bayes'])\n",
    "print(\"Prediction Time: %f\"%prediction_time_container['b_naive_bayes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix of Bernoulli Naive Bayes Classifier output: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[279,  40,   6],\n",
       "       [  3, 236,   4],\n",
       "       [  2,  35, 275]])"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it shows that the accuracy score of our model is 0.954 or 95.4%. \n",
    "#Confusion Matrix is also another way to evaluate the prediction output of a classifier and also to determine the false positive\n",
    "#and false negative, sensitivity, specificity, precision and recall metrics:\n",
    "print(\"Confusion Matrix of Bernoulli Naive Bayes Classifier output: \")\n",
    "sklearn.metrics.confusion_matrix(labels_test,bnb_predictions)\n",
    "#the values on the diagonal show correct predictions where as off-diagonal represent the records that have been misclassified.\n",
    "#also printing the detailed report as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [200,  45,   8],\n",
    "#        [  0, 239,  18],\n",
    "#        [  3,  52, 272]])\n",
    "    \n",
    "# 0.875870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
